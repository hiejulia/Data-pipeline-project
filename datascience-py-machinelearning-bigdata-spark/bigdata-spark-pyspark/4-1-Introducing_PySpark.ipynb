{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing PySpark\n",
    "\n",
    "\n",
    "Python, pandas, scipy, and scikit-learn are all great tools for exploring and learning from small to mid-size datasets.  Like most programming languages, Python is primarily designed to work in a certain way: load data from local disk or from a database into memory, process that data, and store the results back to disk.\n",
    "\n",
    "But what if we want to process data that’s too big for one computer?   In the past, we would mainly avoid this problem by sampling.  As we don’t need to necessarily train a model on a huge dataset, we find that a good statistical sample can often be even better.  That said, there are some problems with this.  How do we know that the sample really reflects a disparate dataset?   Maybe the most important features are in a subset of the data that the sample never sees?\n",
    "\n",
    "Even so, there are certain types of models for which sampling just does not work.  Recommendations is a good example. Recommenders rely on comparing users with other users in evaluating their preferences.  With a subset of the data, there won’t be a cohort of users who are very similar to one another.  Recommenders really need to run on the full dataset, or not at all.\n",
    "\n",
    "So what is the answer?  The answer has been obvious for a long time: split the problem up onto multiple computers.  That said, the problem is easier said than done.   Many problems aren’t easily split up. Even if it is possible to partition out the work, developers often have trouble writing parallel code and end up having to solve a bunch of the complex issues around multiprocessing themselves.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running this notebook\n",
    "\n",
    "This notebook will not run in an ordinary jupyter notebook server.  \n",
    "\n",
    "Here is how we can load pyspark to use Jupyter notebooks.\n",
    "\n",
    "```bash\n",
    "  export PYSPARK_PYTHON=python3 \n",
    "  export PYSPARK_DRIVER_PYTHON=\"jupyter\" \n",
    "  export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook\" \n",
    "  ~/spark/bin/pyspark\n",
    "```\n",
    "\n",
    "The last command will start up a jupyter server similar to what you are used to.  We can then load this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# ds-python-linkedin',\n",
       " 'This is the lab files for ds-python-linkedin',\n",
       " '',\n",
       " '    1-3. [Lab Environment](01_03/start/1-3-LearningNotebooks.ipynb)',\n",
       " '    2-1. [Learning Python](02_01/start/2-1-LearningPython.ipynb)']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.textFile(\"../../README.md\")\n",
    "lines.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Viewing the Spark UI\n",
    "\n",
    "Congradulations!  You just ran your first Spark job.\n",
    "\n",
    "We can view the spark UI at YOURMACHINE:4040.  If you're running on a laptop or desktop, then localhost:4040 will probably work.  let's open up a browser window and go to that location.  You should see something like this:\n",
    "\n",
    "![Spark UI](../../img/pyspark_4040.png \"Spark UI\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've loaded the readme file, let's try a simple operation.  We'll try a filter so that we only load lines with the word \"Spark\" in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linesWithSpark = lines.filter(lambda line: \"spark\" in line)\n",
    "linesWithSpark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
